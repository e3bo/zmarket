---
title: "analysis"
output: rmarkdown::html_vignette
bibliography: bib.json
vignette: >
  %\VignetteIndexEntry{analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(zmarket)
library(dplyr)
library(ggplot2)
library(magrittr)
library(purrr)
```


This document contains a simple exploration and modeling of the Sleep App data provided by Gradient Metrics. Our main sources are Harrel [-@harrel] and Rao [-@rao]. 

# Exploratory data analysis

To examine the experimental design, we calculate an index of all levels of discrete variables using all other discrete variables as grouping variables. For a level of a discrete variable, this index with respect to the level of another grouping variable is equal to its frequency in that level of the grouping variable divided by its frequency across all levels of the grouping variable times 100.


```{r results='asis'}
discrete_vars <- experiment_data %>% select(duration:social_proof)
indtabs <- list()
i <- 1
cat("## Tables of indexes\n Groups levels are on rows and index levels are on columns.")
for(v1 in names(discrete_vars)){
  for(v2 in setdiff(names(discrete_vars), v1)){
    indtabs[[i]] <- index(experiment_data[[v1]], experiment_data[[v2]])
    print(knitr::kable(indtabs[[i]]))
    i <- i + 1
  }
}
```

All index values are close to 100, which means that these variables do not have strong correlations in the overall data set. That is consistent with the description of the experiment as 12 random permutations of a message with 6 attributes.

## Further exploration of design

Next we reproduce a result from the assignment to allow us to comment on it.

```{r}
(levs <- experiment_data %>%
  select(duration:social_proof) %>%
  map(unique) %>%  map(~t(t(.x))))
```

There are 6 attributes in the experiment with between three and six levels. If we use a dummy variable encoding of each attribute, our model would have `r sum(map_dbl(levs, ~nrow(.x) - 1))` parameters. Therefore, it is not reasonable for us to fit such a model for each individual respondent. Therefore, a key modeling challenge will be fit models with relatively large numbers of parameters from relatively small numbers of observations. Our main tools will be to try to pool individuals into homogeneous groups and fit models for those groups and to apply penalties to model parameters to reduce their estimation variance.


## Removal of validation set

To avoid overfitting, we remove 2 tasks from each respondent. We will later use this reserved data for model validation.

```{r}
set.seed(1)
wdat <- experiment_data %>% group_by(response_id) %>% slice_sample(n = 10)

```

## Examination or response variable

To get an idea about the heterogeneity in among respondents, we'll look at some summary statistics of the distribution of their answers across the ten tasks in our working data.

```{r}

rstats <- wdat %>% group_by(response_id) %>% 
  summarise(mean_ans = mean(answer), 
            sd_ans = sd(answer),
            max_ans = max(answer),
            min_ans = min(answer),
            range_ans = max_ans  - min_ans) 

rstats %>% ggplot(aes(x = mean_ans)) + geom_histogram()
rstats %>% ggplot(aes(x = sd_ans)) + geom_histogram()
rstats %>% ggplot(aes(x = range_ans)) + geom_histogram()
rstats %>% ggplot(aes(x = max_ans)) + geom_histogram()
rstats %>% ggplot(aes(x = min_ans)) + geom_histogram()

```

The mean response appears to follow a mixture distribution. A certain fraction of about 250 / 892 of respondents answered 1 ("Very unlikely") in all 12 tasks. Another 70 or so respondents also provided the same answer to all tasks, resulting in about 320 respondents with an standard deviation of zero and a range of 1. Ten individuals had a minimum answer of 4 and must have answered 4 for all tasks. Models for these groups of constant-response individuals could be very simple. They could maximize AIC by having a constant intercept only.

The number of respondents in the sample was a decreasing function of the range of respondents. Ranges of 3 were even rarer than extrapolation of the linear trend from ranges of 0 to 2 would predict. Although rare, the sensitivity of these individuals to some attributes in the experiment may be valuable to know.

On balance the mean answers from the respondents that did not always answer 1 seem to be uniformly distributed.  There may be more variation among groups of individuals than within individuals. This variation maybe explainable in terms of variables included in the survey.

# Modeling

As a starting point, we'll fit a continuation ratio model [@harrel] that assumes homogeneity among respondents. According to Rao [-@rao], this homogeneity assumption is rarely satisfied but accounting for heterogeneity with an overly complex model could produce results that do not generalize beyond the training data and are difficult to interpret. Additionally, our time for this analysis is highly limited. Therefore, we will start with a simple model and build up complexity towards an optimal level. The continuation ratio model is a very simple model that is appropriate for an ordinal response variable.

```{r}

u <- with(wdat, rms::cr.setup(answer))

Y <- u$y
cohort <- u$cohort
crdat <- wdat[u$subs, ] %>% ungroup() %>% select(duration:social_proof)

attach(crdat)
m1 <- rms::lrm(Y ~ cohort + duration + offer + outcome + price + rtb + social_proof)
detach(crdat)

m1
anova(m1)
```


This model does not fit particularly well, with $R^2$ in the single digits. But even so there are several statistically significant variables. The most important appears to be the cohort variable, followed by price and social proof.

We'll use prediction performance on the validation data as our main metric to compare this model with later models.






# References
